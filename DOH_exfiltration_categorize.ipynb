{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2c8532279be5d20f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T21:35:19.009304Z",
     "start_time": "2024-04-26T21:35:15.182139Z"
    }
   },
   "source": [
    "from time import time\n",
    "from enum import Enum\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "\n",
    "from keras.layers import Flatten, Dense, Dropout, Conv2D, \\\n",
    "    MaxPool2D\n",
    "from keras.models import Sequential\n",
    "\n",
    "nest_asyncio.apply()\n",
    "SEED = 1337\n",
    "tf.random.set_seed(SEED)\n",
    "import gc\n",
    "\n",
    "gc.collect()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3faf91d5820d8813",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T21:35:19.016012Z",
     "start_time": "2024-04-26T21:35:19.011341Z"
    }
   },
   "source": [
    "EXPERIMENT_NAME = \"DOH-Exfiltration-detection\"\n",
    "this_dir = Path.cwd()\n",
    "print(this_dir)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "63f886de72dc4c5f",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4c2275836e299872",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T21:35:20.228062Z",
     "start_time": "2024-04-26T21:35:19.018031Z"
    }
   },
   "source": [
    "\n",
    "data_dir = this_dir / \"datasets\"\n",
    "\n",
    "\n",
    "class Label(Enum):\n",
    "    BENIGN = 0\n",
    "    DNSCAT2 = 1\n",
    "    DNS2TCP = 2\n",
    "    IODINE = 3\n",
    "\n",
    "\n",
    "benign_data = pd.read_csv(data_dir / \"CSVs/benign_ca.csv\", nrows=10000)\n",
    "benign_data['label'] = Label.BENIGN.value\n",
    "\n",
    "# Đọc các file CSV khác\n",
    "dnscat2_data = pd.read_csv(data_dir / \"dnscat2.csv\", nrows=10000)\n",
    "dnscat2_data['label'] = Label.DNSCAT2.value\n",
    "dns2tcp_data = pd.read_csv(data_dir / \"dns2tcp.csv\", nrows=10000)\n",
    "dns2tcp_data['label'] = Label.DNS2TCP.value\n",
    "iodine_data = pd.read_csv(data_dir / \"iodine.csv\", nrows=10000)\n",
    "iodine_data['label'] = Label.IODINE.value\n",
    "\n",
    "print('benign_data: ' + str(benign_data.groupby('flow_id').size().count()),\n",
    "      ', dnscat2_data: ' + str(dnscat2_data.groupby('flow_id').size().count()),\n",
    "      ', dns2tcp_data: ' + str(dns2tcp_data.groupby('flow_id').size().count()),\n",
    "      ', iodine_data: ' + str(iodine_data.groupby('flow_id').size().count()))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "85dd10fa8cab22c8",
   "metadata": {},
   "source": [
    "# Split data into 64% train, 16% validate and 20% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "405aefbfd375133",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T21:35:20.236889Z",
     "start_time": "2024-04-26T21:35:20.230101Z"
    }
   },
   "source": [
    "# Split data into train, validate and test with group flow_id\n",
    "def slit_train_validate_test(data, train_percent=.8, validate_percent=.2, seed=None):\n",
    "    np.random.seed(seed)\n",
    "    grouped = data.groupby('flow_id')\n",
    "    arranged = np.arange(grouped.ngroups)\n",
    "    np.random.shuffle(arranged)\n",
    "\n",
    "    train = data[grouped.ngroup().isin(arranged[:int(len(arranged) * train_percent)])]\n",
    "    test = data.drop(train.index)\n",
    "    test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    grouped = train.groupby('flow_id')\n",
    "    arranged = np.arange(grouped.ngroups)\n",
    "    np.random.shuffle(arranged)\n",
    "\n",
    "    temp = train[grouped.ngroup().isin(arranged[:int(len(arranged) * (1 - validate_percent))])]\n",
    "    validate = train.drop(temp.index)\n",
    "    validate.reset_index(drop=True, inplace=True)\n",
    "    train = temp\n",
    "\n",
    "    return train, validate, test"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a8764013b3a47b3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T21:35:20.280811Z",
     "start_time": "2024-04-26T21:35:20.238897Z"
    }
   },
   "source": [
    "# Split benign data into train, validate and test\n",
    "train_benign, validate_benign, test_benign = slit_train_validate_test(benign_data)\n",
    "print(\"train_benign: \", train_benign.groupby('flow_id').size().count(), \", validate_benign: \",\n",
    "      validate_benign.groupby('flow_id').size().count(), \", test_benign: \",\n",
    "      test_benign.groupby('flow_id').size().count())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4673dba03688a03e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T21:35:20.337706Z",
     "start_time": "2024-04-26T21:35:20.281816Z"
    }
   },
   "source": [
    "# Split dnscat2 data into train, validate and test\n",
    "train_dnscat2, validate_dnscat2, test_dnscat2 = slit_train_validate_test(dnscat2_data)\n",
    "print(\"train_dnscat2: \", train_dnscat2.groupby('flow_id').size().count(), \", validate_dnscat2: \",\n",
    "      validate_dnscat2.groupby('flow_id').size().count(), \", test_dnscat2: \",\n",
    "      test_dnscat2.groupby('flow_id').size().count())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e313dc3ad9770f36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T21:35:20.389265Z",
     "start_time": "2024-04-26T21:35:20.339714Z"
    }
   },
   "source": [
    "# Split dns2tcp data into train, validate and test\n",
    "train_dns2tcp, validate_dns2tcp, test_dns2tcp = slit_train_validate_test(dns2tcp_data)\n",
    "print(\"train_dns2tcp: \", train_dns2tcp.groupby('flow_id').size().count(), \", validate_dns2tcp: \",\n",
    "      validate_dns2tcp.groupby('flow_id').size().count(), \", test_dns2tcp: \",\n",
    "      test_dns2tcp.groupby('flow_id').size().count())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "58f4cfd43fdcb48d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T21:35:20.449904Z",
     "start_time": "2024-04-26T21:35:20.390268Z"
    }
   },
   "source": [
    "# Split iodine data into train, validate and test\n",
    "train_iodine, validate_iodine, test_iodine = slit_train_validate_test(iodine_data)\n",
    "print(\"train_iodine: \", train_iodine.groupby('flow_id').size().count(), \", validate_iodine: \",\n",
    "      validate_iodine.groupby('flow_id').size().count(), \", test_iodine: \",\n",
    "      test_iodine.groupby('flow_id').size().count())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "be94895e7fcd0691",
   "metadata": {},
   "source": [
    "# Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "19e2351d1bd93a5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T21:35:21.598090Z",
     "start_time": "2024-04-26T21:35:20.461708Z"
    }
   },
   "source": [
    "# Merge data from all iterations in data list, reset flow id by increasing the last flow id of the previous data\n",
    "def merge_dataframes(dataframes: list):\n",
    "    flow_id = 1\n",
    "    merged_dataframes = []\n",
    "\n",
    "    for df in dataframes:\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        # Change all item flow_id of this flow to new flow_id\n",
    "        for flow in df.groupby('flow_id'):\n",
    "            flow[1]['flow_id'] = flow_id\n",
    "            merged_dataframes.append(flow[1])\n",
    "            flow_id += 1\n",
    "\n",
    "    return pd.concat(merged_dataframes)"
   ],
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_train = merge_dataframes([train_benign, train_dnscat2, train_dns2tcp, train_iodine])\n",
    "data_validate = merge_dataframes([validate_benign, validate_dnscat2, validate_dns2tcp, validate_iodine])\n",
    "data_test = merge_dataframes([test_benign, test_dnscat2, test_dns2tcp, test_iodine])\n",
    "print(\"Train data: \" + str(data_train.groupby('flow_id').size().count()),\n",
    "      \", Validate data: \" + str(data_validate.groupby('flow_id').size().count()),\n",
    "      \", Test data: \" + str(data_test.groupby('flow_id').size().count()))"
   ],
   "id": "7922aa6dead335a9",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Get label",
   "id": "8fa702ea31eddceb"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e4b10a58665255bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T21:35:21.615425Z",
     "start_time": "2024-04-26T21:35:21.607346Z"
    }
   },
   "source": [
    "def most_frequent(flow):\n",
    "    return max(set(flow), key=flow.count)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3e683967aad8bca5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T21:35:21.712151Z",
     "start_time": "2024-04-26T21:35:21.617441Z"
    }
   },
   "source": [
    "def get_label(data):\n",
    "    grouped = data.groupby('flow_id')['label'].apply(list).to_dict()\n",
    "\n",
    "    label = []\n",
    "    for flow in grouped:\n",
    "        label.append(most_frequent(grouped[flow]))\n",
    "\n",
    "    return np.array(label)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b196b19444d13c79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T21:35:21.766878Z",
     "start_time": "2024-04-26T21:35:21.714158Z"
    }
   },
   "source": [
    "label_train = np.array(get_label(data_train))\n",
    "label_validate = np.array(get_label(data_validate))\n",
    "label_test = np.array(get_label(data_test))\n",
    "\n",
    "print('Label train: ', len(label_train), ', Label validate: ', len(label_validate), ', Label test: ', len(label_test))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "61fc0dbd455de603",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T21:35:21.775117Z",
     "start_time": "2024-04-26T21:35:21.768895Z"
    }
   },
   "source": [
    "data_train_without_flow = data_train.drop('flow_id', axis=1)\n",
    "data_validate_without_flow = data_validate.drop('flow_id', axis=1)\n",
    "data_test_without_flow = data_test.drop('flow_id', axis=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "93ba87f58f784c67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T21:35:21.922888Z",
     "start_time": "2024-04-26T21:35:21.777144Z"
    }
   },
   "source": [
    "NUM_FEATURE = len(data_test_without_flow.columns) - 1\n",
    "NUM_CLASSES = len(np.unique(data_test_without_flow['label']))\n",
    "NUM_PACKETS_PER_FLOW = 10\n",
    "\n",
    "print('Number of features: ', NUM_FEATURE, ', Number of classes: ', NUM_CLASSES, ', Number of packets per flow: ',\n",
    "      NUM_PACKETS_PER_FLOW)"
   ],
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x_train = np.array(\n",
    "    (data_train_without_flow.drop('label', axis=1).to_numpy() / 255).reshape(-1, NUM_PACKETS_PER_FLOW, NUM_FEATURE))\n",
    "x_validate = np.expand_dims(\n",
    "    (data_validate_without_flow.drop('label', axis=1).to_numpy() / 255).reshape(-1, NUM_PACKETS_PER_FLOW, NUM_FEATURE),\n",
    "    axis=-1)\n",
    "x_test = np.expand_dims(\n",
    "    (data_test_without_flow.drop('label', axis=1).to_numpy() / 255).reshape(-1, NUM_PACKETS_PER_FLOW, NUM_FEATURE),\n",
    "    axis=-1)\n",
    "\n",
    "print('x_train shape: ', x_train.shape, ', x_validate shape: ', x_validate.shape, ', x_test shape: ', x_test.shape)"
   ],
   "id": "a4457c75db77cd92",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model training",
   "id": "3f457492cec3996e"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f630a008b393c78b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T21:35:22.127801Z",
     "start_time": "2024-04-26T21:35:21.935093Z"
    }
   },
   "source": [
    "def create_keras_model(num_packet_per_flow, num_features, num_classes):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=32, kernel_size=(5, 5), padding='Same',\n",
    "                     activation='relu', input_shape=(num_packet_per_flow, num_features, 1)))\n",
    "    model.add(Conv2D(filters=32, kernel_size=(5, 5), padding='Same',\n",
    "                     activation='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='Same',\n",
    "                     activation='relu'))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='Same',\n",
    "                     activation='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation=\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "    return model"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fde12118b8a6f73c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T21:38:44.657582Z",
     "start_time": "2024-04-26T21:35:22.128871Z"
    }
   },
   "source": [
    "# Initialize the model\n",
    "client_lr = 3e-4\n",
    "NUM_ROUNDS = 300\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "model = create_keras_model(NUM_PACKETS_PER_FLOW, NUM_FEATURE, NUM_CLASSES)\n",
    "model.summary()"
   ],
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(\n",
    "    learning_rate=client_lr), loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['sparse_categorical_accuracy'])\n",
    "start = time()\n",
    "\n",
    "history = model.fit(x_train, label_train, epochs=5, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                    validation_data=(x_validate, label_validate))\n",
    "end = time() - start\n",
    "\n",
    "print(f'Training time: {end} seconds')"
   ],
   "id": "68aceb3ee7c7e8fe",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluate the model",
   "id": "b57e97b0d38b0078"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f98f3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T21:38:46.395604Z",
     "start_time": "2024-04-26T21:38:44.664191Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "result_dir = this_dir / 'results'\n",
    "output_dir = result_dir / EXPERIMENT_NAME\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700afefd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T21:38:47.110893Z",
     "start_time": "2024-04-26T21:38:46.396614Z"
    }
   },
   "source": [
    "best_model_accuracy = history.history['sparse_categorical_accuracy'][np.argmin(history.history['loss'])]\n",
    "_, test_acc = model.evaluate(x_test, label_test, verbose=2, batch_size=BATCH_SIZE)\n",
    "train_val = str(round(best_model_accuracy * 100)) + \"_\" + str(round(test_acc * 100))\n",
    "\n",
    "print(train_val)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8b09f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T21:38:47.118807Z",
     "start_time": "2024-04-26T21:38:47.112912Z"
    }
   },
   "source": [
    "import keras\n",
    "\n",
    "keras.saving.save_model(model, output_dir / 'model.keras')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135804b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T21:38:47.128152Z",
     "start_time": "2024-04-26T21:38:47.120815Z"
    }
   },
   "source": [
    "with open(output_dir / 'parameters.txt', 'w') as f:\n",
    "    print('client_lr: {}\\nEpochs: {}\\nBATCH_SIZE: {}'.format(\n",
    "        client_lr, NUM_ROUNDS, BATCH_SIZE), file=f)\n",
    "    f.close()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47498c23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T21:38:49.074566Z",
     "start_time": "2024-04-26T21:38:47.130174Z"
    }
   },
   "source": [
    "def sec_to_hours(seconds):\n",
    "    a = seconds // 3600\n",
    "    b = (seconds % 3600) // 60\n",
    "    c = (seconds % 3600) % 60\n",
    "    d = \"{:.0f} hours {:.0f} mins {:.0f} seconds\".format(a, b, c)\n",
    "    return d\n",
    "\n",
    "\n",
    "total_time = \"Time: {}\".format(sec_to_hours(end))\n",
    "\n",
    "text_file = open(output_dir / \"time.txt\", \"w\")\n",
    "n = text_file.write(total_time)\n",
    "text_file.close()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85053eb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T21:38:49.081341Z",
     "start_time": "2024-04-26T21:38:49.075617Z"
    }
   },
   "source": [
    "start = time()\n",
    "predictions = model.predict(\n",
    "    x_test, verbose=2, batch_size=BATCH_SIZE)\n",
    "end = time() - start\n",
    "text_file = open(output_dir / \"time.txt\", \"a\")\n",
    "text_file.write(f'\\nPredict time: {sec_to_hours(end)}')\n",
    "text_file.close()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d5d7ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T21:38:49.259360Z",
     "start_time": "2024-04-26T21:38:49.082355Z"
    }
   },
   "source": "flow_pred = np.argmax(predictions, axis=-1)",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0e4a6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T21:38:49.884531Z",
     "start_time": "2024-04-26T21:38:49.263366Z"
    }
   },
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "CLASSES_LIST = ['Benign', 'DNSCat2', 'DNS2TCP', 'Iodine']\n",
    "\n",
    "with open(output_dir / 'metrics.txt', 'w') as f:\n",
    "    # importing accuracy_score, precision_score, recall_score, f1_score\n",
    "    print('\\nAccuracy: {:.2f}\\n'.format(\n",
    "        accuracy_score(label_test, flow_pred)), file=f)\n",
    "\n",
    "    print('Micro Precision: {:.2f}'.format(\n",
    "        precision_score(label_test, flow_pred, average='micro')), file=f)\n",
    "    print('Micro Recall: {:.2f}'.format(\n",
    "        recall_score(label_test, flow_pred, average='micro')), file=f)\n",
    "    print(\n",
    "        'Micro F1-score: {:.2f}\\n'.format(f1_score(label_test, flow_pred, average='micro')), file=f)\n",
    "\n",
    "    print('Macro Precision: {:.2f}'.format(\n",
    "        precision_score(label_test, flow_pred, average='macro')), file=f)\n",
    "    print('Macro Recall: {:.2f}'.format(\n",
    "        recall_score(label_test, flow_pred, average='macro')), file=f)\n",
    "    print(\n",
    "        'Macro F1-score: {:.2f}\\n'.format(f1_score(label_test, flow_pred, average='macro')), file=f)\n",
    "\n",
    "    print('Weighted Precision: {:.2f}'.format(\n",
    "        precision_score(label_test, flow_pred, average='weighted')), file=f)\n",
    "    print('Weighted Recall: {:.2f}'.format(\n",
    "        recall_score(label_test, flow_pred, average='weighted')), file=f)\n",
    "    print(\n",
    "        'Weighted F1-score: {:.2f}'.format(f1_score(label_test, flow_pred, average='weighted')), file=f)\n",
    "\n",
    "    print('\\nClassification Report\\n', file=f)\n",
    "    print(classification_report(label_test, flow_pred, target_names=CLASSES_LIST), file=f)\n",
    "    f.close()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bbe13c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T21:38:50.522523Z",
     "start_time": "2024-04-26T21:38:49.886536Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ConfusionMatrixDisplay.from_predictions(label_test, flow_pred, display_labels=CLASSES_LIST, xticks_rotation='vertical',\n",
    "                                        ax=ax, colorbar=False)\n",
    "plt.savefig(output_dir / 'ConfusionMatrix.pdf', bbox_inches=\"tight\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af49d41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T21:38:50.945783Z",
     "start_time": "2024-04-26T21:38:50.524536Z"
    }
   },
   "source": [
    "from matplotlib.ticker import PercentFormatter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def cm_analysis(y_true, y_pred, filename, labels, classes, ymap=None, figsize=(17, 17)):\n",
    "    \"\"\"\n",
    "    Generate matrix plot of confusion matrix with pretty annotations.\n",
    "    The plot image is saved to disk.\n",
    "    args: \n",
    "      y_true:    true label of the data, with shape (nsamples,)\n",
    "      y_pred:    prediction of the data, with shape (nsamples,)\n",
    "      filename:  filename of figure file to save\n",
    "      labels:    string array, name the order of class labels in the confusion matrix.\n",
    "                 use `clf.classes_` if using scikit-learn models.\n",
    "                 with shape (nclass,).\n",
    "      classes:   aliases for the labels. String array to be shown in the cm plot.\n",
    "      ymap:      dict: any -> string, length == nclass.\n",
    "                 if not None, map the labels & ys to more understandable strings.\n",
    "                 Caution: original y_true, y_pred and labels must align.\n",
    "      figsize:   the size of the figure plotted.\n",
    "    \"\"\"\n",
    "    sns.set(font_scale=1)\n",
    "\n",
    "    if ymap is not None:\n",
    "        y_pred = [ymap[yi] for yi in y_pred]\n",
    "        y_true = [ymap[yi] for yi in y_true]\n",
    "        labels = [ymap[yi] for yi in labels]\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "    cm_perc = cm / cm_sum.astype(float) * 100\n",
    "    annot = np.empty_like(cm).astype(str)\n",
    "    nrows, ncols = cm.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            c = cm[i, j]\n",
    "            p = cm_perc[i, j]\n",
    "            if i == j:\n",
    "                s = cm_sum[i]\n",
    "                annot[i, j] = '%.2f%%\\n%d/%d' % (p, c, s[0])\n",
    "            #elif c == 0:\n",
    "            #    annot[i, j] = ''\n",
    "            else:\n",
    "                annot[i, j] = '%.2f%%\\n%d' % (p, c)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels, normalize='true')\n",
    "    cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "    cm = cm * 100\n",
    "    cm.index.name = 'True Label'\n",
    "    cm.columns.name = 'Predicted Label'\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    plt.yticks(va='center')\n",
    "\n",
    "    sns.heatmap(cm, annot=annot, fmt='', ax=ax, xticklabels=classes, cbar=True, cbar_kws={'format': PercentFormatter()},\n",
    "                yticklabels=classes, cmap=\"Blues\")\n",
    "    plt.savefig(filename, bbox_inches='tight')\n",
    "\n",
    "\n",
    "cm_analysis(y_true=label_test, y_pred=flow_pred, filename=output_dir / 'ConfusionMatrix_nom.pdf', labels=[0, 1, 2, 3],\n",
    "            classes=CLASSES_LIST, figsize=(12, 10))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d574c91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T21:38:51.303791Z",
     "start_time": "2024-04-26T21:38:50.947790Z"
    }
   },
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['sparse_categorical_accuracy'], label='accuracy')\n",
    "plt.plot(\n",
    "    history.history['val_sparse_categorical_accuracy'], label='val_accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.savefig(output_dir / \"normal_model_Accuracy.pdf\")"
   ],
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig(output_dir / \"normal_model_Loss.pdf\")\n"
   ],
   "id": "d8aa5873a5a165aa",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
