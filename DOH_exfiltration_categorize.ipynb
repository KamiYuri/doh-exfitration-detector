{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8532279be5d20f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T08:50:42.282612Z",
     "start_time": "2024-06-11T08:50:34.848149Z"
    }
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from enum import Enum\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "\n",
    "from keras.layers import Flatten, Dense, Dropout, Conv2D, \\\n",
    "    MaxPool2D\n",
    "from keras.models import Sequential\n",
    "\n",
    "nest_asyncio.apply()\n",
    "SEED = 1337\n",
    "tf.random.set_seed(SEED)\n",
    "import gc\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faf91d5820d8813",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T08:50:42.288332Z",
     "start_time": "2024-06-11T08:50:42.284376Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "this_dir = Path.cwd()\n",
    "print(this_dir)\n",
    "\n",
    "# NUM_FEATURE = len(data_test_without_flow.columns) - 1\n",
    "NUM_FEATURE = 512\n",
    "NUM_PACKETS_PER_FLOW = 15\n",
    "EPOCHS = 5\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "EXPERIMENT_NAME = f\"DED-{NUM_PACKETS_PER_FLOW}-{NUM_FEATURE}\"\n",
    "\n",
    "print('Number of features: ', NUM_FEATURE, ', Number of classes: ', NUM_CLASSES, ', Number of packets per flow: ',\n",
    "      NUM_PACKETS_PER_FLOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f886de72dc4c5f",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2275836e299872",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T08:50:43.467370Z",
     "start_time": "2024-06-11T08:50:42.289822Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# data_dir = this_dir / \"dataset\" / \"csv\"\n",
    "\n",
    "\n",
    "# class Label(Enum):\n",
    "#     BENIGN = 0\n",
    "#     DNSCAT2 = 1\n",
    "#     DNS2TCP = 2\n",
    "#     IODINE = 3\n",
    "\n",
    "\n",
    "# # NROWS = 100\n",
    "# NROWS = 102980\n",
    "\n",
    "# # Đọc file CSV benign\n",
    "# benign_dir = data_dir / \"benign\"\n",
    "\n",
    "# benign_data = pd.read_csv(benign_dir / \"benign_chrome_google.csv\", nrows=NROWS)\n",
    "# benign_data['label'] = Label.BENIGN.value\n",
    "\n",
    "# # Đọc các file CSV khác\n",
    "# malicious_dir = data_dir / \"malicious\"\n",
    "\n",
    "# dnscat2_data = pd.read_csv(malicious_dir / \"dnscat2.csv\", nrows=NROWS)\n",
    "# dnscat2_data['label'] = Label.DNSCAT2.value\n",
    "# dns2tcp_data = pd.read_csv(malicious_dir / \"dns2tcp.csv\", nrows=NROWS)\n",
    "# dns2tcp_data['label'] = Label.DNS2TCP.value\n",
    "# iodine_data = pd.read_csv(malicious_dir / \"iodine.csv\", nrows=NROWS)\n",
    "# iodine_data['label'] = Label.IODINE.value\n",
    "\n",
    "# print('benign_data: ' + str(benign_data.groupby('flow_id').size().count()),\n",
    "#       ', dnscat2_data: ' + str(dnscat2_data.groupby('flow_id').size().count()),\n",
    "#       ', dns2tcp_data: ' + str(dns2tcp_data.groupby('flow_id').size().count()),\n",
    "#       ', iodine_data: ' + str(iodine_data.groupby('flow_id').size().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0ec51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_dir = this_dir / \"dataset\" / \"csv\"\n",
    "\n",
    "\n",
    "class Label(Enum):\n",
    "    BENIGN = 0\n",
    "    DNSCAT2 = 1\n",
    "    DNS2TCP = 2\n",
    "    IODINE = 3\n",
    "\n",
    "\n",
    "flow_number = 100\n",
    "\n",
    "\n",
    "benign_data = pd.read_csv(data_dir / \"benign_reduce.csv\", nrows=flow_number * 20)\n",
    "benign_data['label'] = Label.BENIGN.value\n",
    "dnscat2_data = pd.read_csv(data_dir / \"dnscat2_reduce.csv\", nrows=flow_number * 20)\n",
    "dnscat2_data['label'] = Label.DNSCAT2.value\n",
    "dns2tcp_data = pd.read_csv(data_dir / \"dns2tcp_reduce.csv\", nrows=flow_number * 20)\n",
    "dns2tcp_data['label'] = Label.DNS2TCP.value\n",
    "iodine_data = pd.read_csv(data_dir / \"iodine_reduce.csv\", nrows=flow_number * 20)\n",
    "iodine_data['label'] = Label.IODINE.value\n",
    "\n",
    "print('benign_data: ' + str(benign_data.groupby('flow_id').size().count()),\n",
    "      ', dnscat2_data: ' + str(dnscat2_data.groupby('flow_id').size().count()),\n",
    "      ', dns2tcp_data: ' + str(dns2tcp_data.groupby('flow_id').size().count()),\n",
    "      ', iodine_data: ' + str(iodine_data.groupby('flow_id').size().count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dd10fa8cab22c8",
   "metadata": {},
   "source": [
    "# Split data into train, validate and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dd93130ecc29d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T08:50:43.628978Z",
     "start_time": "2024-06-11T08:50:43.468547Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "\n",
    "def split(dataset: pd.DataFrame, split_ratio: float = 0.8) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Split a dataset into a training and test dataset.\n",
    "    :param dataset: The dataset to split.\n",
    "    :param split_ratio: The ratio of the split.\n",
    "    :return: A tuple containing the training and test datasets.\n",
    "    \"\"\"\n",
    "    first_inds, next_inds = next(\n",
    "        GroupShuffleSplit(train_size=split_ratio, n_splits=2, random_state=42).split(dataset,\n",
    "                                                                                     groups=dataset['flow_id']))\n",
    "\n",
    "    return dataset.iloc[first_inds], dataset.iloc[next_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405aefbfd375133",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T08:50:43.646683Z",
     "start_time": "2024-06-11T08:50:43.635698Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Split data into train, validate and test with group flow_id\n",
    "# def slit_train_validate_test(data, train_percent=.8, validate_percent=.2, seed=None):\n",
    "#     np.random.seed(seed)\n",
    "#     grouped = data.groupby('flow_id')\n",
    "#     arranged = np.arange(grouped.ngroups)\n",
    "#     np.random.shuffle(arranged)\n",
    "# \n",
    "#     train = data[grouped.ngroup().isin(arranged[:int(len(arranged) * train_percent)])]\n",
    "#     test = data.drop(train.index)\n",
    "#     test.reset_index(drop=True, inplace=True)\n",
    "# \n",
    "#     grouped = train.groupby('flow_id')\n",
    "#     arranged = np.arange(grouped.ngroups)\n",
    "#     np.random.shuffle(arranged)\n",
    "# \n",
    "#     temp = train[grouped.ngroup().isin(arranged[:int(len(arranged) * (1 - validate_percent))])]\n",
    "#     validate = train.drop(temp.index)\n",
    "#     validate.reset_index(drop=True, inplace=True)\n",
    "#     train = temp\n",
    "# \n",
    "#     return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8764013b3a47b3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T08:50:43.736515Z",
     "start_time": "2024-06-11T08:50:43.650382Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split benign data into train, validate and test\n",
    "train_benign, test_benign = split(benign_data, 0.8)\n",
    "train_benign, validate_benign = split(train_benign, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb86945",
   "metadata": {},
   "outputs": [],
   "source": [
    "del benign_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4673dba03688a03e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T08:50:43.741450Z",
     "start_time": "2024-06-11T08:50:43.737894Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split dnscat2 data into train, validate and test\n",
    "# train_dnscat2, validate_dnscat2, test_dnscat2 = slit_train_validate_test(dnscat2_data)\n",
    "# print(\"train_dnscat2: \", train_dnscat2.groupby('flow_id').size().count(), \", validate_dnscat2: \",\n",
    "#       validate_dnscat2.groupby('flow_id').size().count(), \", test_dnscat2: \",\n",
    "#       test_dnscat2.groupby('flow_id').size().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59ea3e8c72eb069",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T08:50:43.780491Z",
     "start_time": "2024-06-11T08:50:43.743066Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dnscat2, test_dnscat2 = split(dnscat2_data, 0.8)\n",
    "train_dnscat2, validate_dnscat2 = split(train_dnscat2, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd4812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del dnscat2_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e313dc3ad9770f36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T08:50:43.833962Z",
     "start_time": "2024-06-11T08:50:43.782196Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split dns2tcp data into train, validate and test\n",
    "train_dns2tcp, test_dns2tcp = split(dns2tcp_data, 0.8)\n",
    "train_dns2tcp, validate_dns2tcp = split(train_dns2tcp, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a82fa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del dns2tcp_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f4cfd43fdcb48d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T08:50:43.875824Z",
     "start_time": "2024-06-11T08:50:43.837563Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split iodine data into train, validate and test\n",
    "train_iodine, test_iodine = split(iodine_data, 0.8)\n",
    "train_iodine, validate_iodine = split(train_iodine, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca996005",
   "metadata": {},
   "outputs": [],
   "source": [
    "del iodine_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be94895e7fcd0691",
   "metadata": {},
   "source": [
    "# Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e2351d1bd93a5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T08:50:43.880342Z",
     "start_time": "2024-06-11T08:50:43.877224Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Merge data from all iterations in data list, reset flow id by increasing the last flow id of the previous data\n",
    "# def merge_dataframes(dataframes: list):\n",
    "#     flow_id = 1\n",
    "#     merged_dataframes = []\n",
    "# \n",
    "#     for df in dataframes:\n",
    "#         df.reset_index(drop=True, inplace=True)\n",
    "#         # Change all item flow_id of this flow to new flow_id\n",
    "#         for flow in df.groupby('flow_id'):\n",
    "#             flow[1]['flow_id'] = flow_id\n",
    "#             merged_dataframes.append(flow[1])\n",
    "#             flow_id += 1\n",
    "# \n",
    "#     return pd.concat(merged_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7922aa6dead335a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T08:50:43.932698Z",
     "start_time": "2024-06-11T08:50:43.881802Z"
    }
   },
   "outputs": [],
   "source": [
    "# data_train = merge_dataframes([train_benign, train_dnscat2, train_dns2tcp, train_iodine])\n",
    "# data_validate = merge_dataframes([validate_benign, validate_dnscat2, validate_dns2tcp, validate_iodine])\n",
    "# data_test = merge_dataframes([test_benign, test_dnscat2, test_dns2tcp, test_iodine])\n",
    "# print(\"Train data: \" + str(data_train.groupby('flow_id').size().count()),\n",
    "#       \", Validate data: \" + str(data_validate.groupby('flow_id').size().count()),\n",
    "#       \", Test data: \" + str(data_test.groupby('flow_id').size().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9ae9d9daa1eef4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T08:50:43.979076Z",
     "start_time": "2024-06-11T08:50:43.935509Z"
    }
   },
   "outputs": [],
   "source": [
    "data_train = pd.concat([train_benign, train_dnscat2, train_dns2tcp, train_iodine], ignore_index=True, sort=False,\n",
    "                       axis=0)\n",
    "data_validate = pd.concat([validate_benign, validate_dnscat2, validate_dns2tcp, validate_iodine], ignore_index=True,\n",
    "                          sort=False, axis=0)\n",
    "data_test = pd.concat([test_benign, test_dnscat2, test_dns2tcp, test_iodine], ignore_index=True, sort=False, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa702ea31eddceb",
   "metadata": {},
   "source": [
    "# Get label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b10a58665255bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T08:50:43.983766Z",
     "start_time": "2024-06-11T08:50:43.980435Z"
    }
   },
   "outputs": [],
   "source": [
    "def most_frequent(flow):\n",
    "    return max(set(flow), key=flow.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e683967aad8bca5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T08:50:43.989266Z",
     "start_time": "2024-06-11T08:50:43.985423Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_label(data):\n",
    "    grouped = data.groupby('flow_id')['label'].apply(list).to_dict()\n",
    "\n",
    "    label = []\n",
    "    for flow in grouped:\n",
    "        label.append(most_frequent(grouped[flow]))\n",
    "\n",
    "    return np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b196b19444d13c79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T08:50:44.161285Z",
     "start_time": "2024-06-11T08:50:43.990878Z"
    }
   },
   "outputs": [],
   "source": [
    "label_train = np.array(get_label(data_train))\n",
    "label_validate = np.array(get_label(data_validate))\n",
    "label_test = np.array(get_label(data_test))\n",
    "\n",
    "print('Label train: ', len(label_train), ', Label validate: ', len(label_validate), ', Label test: ', len(label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a146a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop number of rows per flow to NUM_PACKETS_PER_FLOW\n",
    "def drop_rows(data):\n",
    "    return data.groupby('flow_id').head(NUM_PACKETS_PER_FLOW)\n",
    "\n",
    "data_train = drop_rows(data_train)\n",
    "data_validate = drop_rows(data_validate)\n",
    "data_test = drop_rows(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fc0dbd455de603",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T08:50:44.198641Z",
     "start_time": "2024-06-11T08:50:44.162764Z"
    }
   },
   "outputs": [],
   "source": [
    "data_train_without_flow = data_train.drop('flow_id', axis=1)\n",
    "data_validate_without_flow = data_validate.drop('flow_id', axis=1)\n",
    "data_test_without_flow = data_test.drop('flow_id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4457c75db77cd92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T08:50:44.335290Z",
     "start_time": "2024-06-11T08:50:44.207802Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = np.array(\n",
    "    (data_train_without_flow.drop('label', axis=1).iloc[:, :NUM_FEATURE].to_numpy() / 255).reshape(-1, NUM_PACKETS_PER_FLOW, NUM_FEATURE))\n",
    "x_validate = np.expand_dims(\n",
    "    (data_validate_without_flow.drop('label', axis=1).iloc[:, :NUM_FEATURE].to_numpy() / 255).reshape(-1, NUM_PACKETS_PER_FLOW, NUM_FEATURE),\n",
    "    axis=-1)\n",
    "x_test = np.expand_dims(\n",
    "    (data_test_without_flow.drop('label', axis=1).iloc[:, :NUM_FEATURE].to_numpy() / 255).reshape(-1, NUM_PACKETS_PER_FLOW, NUM_FEATURE),\n",
    "    axis=-1)\n",
    "\n",
    "print('x_train shape: ', x_train.shape, ', x_validate shape: ', x_validate.shape, ', x_test shape: ', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a154483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_train_without_flow, data_validate_without_flow, data_test_without_flow\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f457492cec3996e",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f630a008b393c78b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T08:50:44.343235Z",
     "start_time": "2024-06-11T08:50:44.336472Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_keras_model(num_packet_per_flow, num_features, num_classes):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=32, kernel_size=(5, 5), padding='Same',\n",
    "                     activation='relu', input_shape=(num_packet_per_flow, num_features, 1)))\n",
    "    model.add(Conv2D(filters=32, kernel_size=(5, 5), padding='Same',\n",
    "                     activation='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='Same',\n",
    "                     activation='relu'))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='Same',\n",
    "                     activation='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation=\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde12118b8a6f73c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T08:50:44.582978Z",
     "start_time": "2024-06-11T08:50:44.344886Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "client_lr = 3e-4\n",
    "NUM_ROUNDS = 300\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "model = create_keras_model(NUM_PACKETS_PER_FLOW, NUM_FEATURE, NUM_CLASSES)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aceb3ee7c7e8fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T08:55:08.168911Z",
     "start_time": "2024-06-11T08:50:44.584739Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(\n",
    "    learning_rate=client_lr), loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['sparse_categorical_accuracy'])\n",
    "start = time()\n",
    "\n",
    "history = model.fit(x_train, label_train, epochs=EPOCHS, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                    validation_data=(x_validate, label_validate))\n",
    "end = time() - start\n",
    "\n",
    "print(f'Training time: {end} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57e97b0d38b0078",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f98f3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T08:55:08.174988Z",
     "start_time": "2024-06-11T08:55:08.170655Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "result_dir = this_dir / 'results'\n",
    "output_dir = result_dir / EXPERIMENT_NAME\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700afefd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T08:55:09.974423Z",
     "start_time": "2024-06-11T08:55:08.177003Z"
    }
   },
   "outputs": [],
   "source": [
    "best_model_accuracy = history.history['sparse_categorical_accuracy'][np.argmin(history.history['loss'])]\n",
    "_, test_acc = model.evaluate(x_validate, label_validate, verbose=2, batch_size=BATCH_SIZE)\n",
    "train_val = str(round(best_model_accuracy * 100)) + \"_\" + str(round(test_acc * 100))\n",
    "\n",
    "print(train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8b09f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T08:55:10.234441Z",
     "start_time": "2024-06-11T08:55:09.976136Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "keras.saving.save_model(model, output_dir / 'model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135804b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T08:55:10.244095Z",
     "start_time": "2024-06-11T08:55:10.236714Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(output_dir / 'parameters.txt', 'w') as f:\n",
    "    print('client_lr: {}\\nEpochs: {}\\nBATCH_SIZE: {}'.format(\n",
    "        client_lr, NUM_ROUNDS, BATCH_SIZE), file=f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47498c23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T08:55:10.450747Z",
     "start_time": "2024-06-11T08:55:10.248194Z"
    }
   },
   "outputs": [],
   "source": [
    "def sec_to_hours(seconds):\n",
    "    a = seconds // 3600\n",
    "    b = (seconds % 3600) // 60\n",
    "    c = (seconds % 3600) % 60\n",
    "    d = \"{:.0f} hours {:.0f} mins {:.0f} seconds\".format(a, b, c)\n",
    "    return d\n",
    "\n",
    "\n",
    "total_time = \"Time: {}\".format(sec_to_hours(end))\n",
    "\n",
    "text_file = open(output_dir / \"time.txt\", \"w\")\n",
    "n = text_file.write(total_time)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85053eb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T08:55:12.407385Z",
     "start_time": "2024-06-11T08:55:10.455038Z"
    }
   },
   "outputs": [],
   "source": [
    "start = time()\n",
    "predictions = model.predict(\n",
    "    x_test, verbose=2, batch_size=BATCH_SIZE)\n",
    "end = time() - start\n",
    "text_file= open(output_dir / \"time.txt\", \"a\")\n",
    "text_file.write(f'\\nPredict time: {sec_to_hours(end)}')\n",
    "text_file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d5d7ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T08:55:12.412165Z",
     "start_time": "2024-06-11T08:55:12.408763Z"
    }
   },
   "outputs": [],
   "source": [
    "flow_pred = np.argmax(predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0e4a6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T08:55:12.451704Z",
     "start_time": "2024-06-11T08:55:12.414249Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "CLASSES_LIST = ['Benign', 'DNSCat2', 'DNS2TCP', 'Iodine']\n",
    "\n",
    "with open(output_dir / 'metrics.txt', 'w') as f:\n",
    "    # importing accuracy_score, precision_score, recall_score, f1_score\n",
    "    print('\\nAccuracy: {:.2f}\\n'.format(\n",
    "        accuracy_score(label_test, flow_pred)), file=f)\n",
    "\n",
    "    print('Micro Precision: {:.2f}'.format(\n",
    "        precision_score(label_test, flow_pred, average='micro')), file=f)\n",
    "    print('Micro Recall: {:.2f}'.format(\n",
    "        recall_score(label_test, flow_pred, average='micro')), file=f)\n",
    "    print(\n",
    "        'Micro F1-score: {:.2f}\\n'.format(f1_score(label_test, flow_pred, average='micro')), file=f)\n",
    "\n",
    "    print('Macro Precision: {:.2f}'.format(\n",
    "        precision_score(label_test, flow_pred, average='macro')), file=f)\n",
    "    print('Macro Recall: {:.2f}'.format(\n",
    "        recall_score(label_test, flow_pred, average='macro')), file=f)\n",
    "    print(\n",
    "        'Macro F1-score: {:.2f}\\n'.format(f1_score(label_test, flow_pred, average='macro')), file=f)\n",
    "\n",
    "    print('Weighted Precision: {:.2f}'.format(\n",
    "        precision_score(label_test, flow_pred, average='weighted')), file=f)\n",
    "    print('Weighted Recall: {:.2f}'.format(\n",
    "        recall_score(label_test, flow_pred, average='weighted')), file=f)\n",
    "    print(\n",
    "        'Weighted F1-score: {:.2f}'.format(f1_score(label_test, flow_pred, average='weighted')), file=f)\n",
    "\n",
    "    print('\\nClassification Report\\n', file=f)\n",
    "    print(classification_report(label_test, flow_pred, target_names=CLASSES_LIST), file=f)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bbe13c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T08:55:12.932552Z",
     "start_time": "2024-06-11T08:55:12.453809Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ConfusionMatrixDisplay.from_predictions(label_test, flow_pred, display_labels=CLASSES_LIST, xticks_rotation='vertical',\n",
    "                                        ax=ax, colorbar=False)\n",
    "plt.savefig(output_dir / 'ConfusionMatrix.pdf', bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af49d41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T08:55:13.434805Z",
     "start_time": "2024-06-11T08:55:12.934301Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.ticker import PercentFormatter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def cm_analysis(y_true, y_pred, filename, labels, classes, ymap=None, figsize=(17, 17)):\n",
    "    \"\"\"\n",
    "    Generate matrix plot of confusion matrix with pretty annotations.\n",
    "    The plot image is saved to disk.\n",
    "    args: \n",
    "      y_true:    true label of the data, with shape (nsamples,)\n",
    "      y_pred:    prediction of the data, with shape (nsamples,)\n",
    "      filename:  filename of figure file to save\n",
    "      labels:    string array, name the order of class labels in the confusion matrix.\n",
    "                 use `clf.classes_` if using scikit-learn models.\n",
    "                 with shape (nclass,).\n",
    "      classes:   aliases for the labels. String array to be shown in the cm plot.\n",
    "      ymap:      dict: any -> string, length == nclass.\n",
    "                 if not None, map the labels & ys to more understandable strings.\n",
    "                 Caution: original y_true, y_pred and labels must align.\n",
    "      figsize:   the size of the figure plotted.\n",
    "    \"\"\"\n",
    "    sns.set(font_scale=1)\n",
    "\n",
    "    if ymap is not None:\n",
    "        y_pred = [ymap[yi] for yi in y_pred]\n",
    "        y_true = [ymap[yi] for yi in y_true]\n",
    "        labels = [ymap[yi] for yi in labels]\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "    cm_perc = cm / cm_sum.astype(float) * 100\n",
    "    annot = np.empty_like(cm).astype(str)\n",
    "    nrows, ncols = cm.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            c = cm[i, j]\n",
    "            p = cm_perc[i, j]\n",
    "            if i == j:\n",
    "                s = cm_sum[i]\n",
    "                annot[i, j] = '%.2f%%\\n%d/%d' % (p, c, s[0])\n",
    "            #elif c == 0:\n",
    "            #    annot[i, j] = ''\n",
    "            else:\n",
    "                annot[i, j] = '%.2f%%\\n%d' % (p, c)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels, normalize='true')\n",
    "    cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "    cm = cm * 100\n",
    "    cm.index.name = 'True Label'\n",
    "    cm.columns.name = 'Predicted Label'\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    plt.yticks(va='center')\n",
    "\n",
    "    sns.heatmap(cm, annot=annot, fmt='', ax=ax, xticklabels=classes, cbar=True, cbar_kws={'format': PercentFormatter()},\n",
    "                yticklabels=classes, cmap=\"Blues\")\n",
    "    # plt.savefig(filename, bbox_inches='tight')\n",
    "\n",
    "\n",
    "cm_analysis(y_true=label_test, y_pred=flow_pred, filename=output_dir / 'ConfusionMatrix_nom.pdf', labels=[0, 1, 2, 3],\n",
    "            classes=CLASSES_LIST, figsize=(12, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d574c91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T08:55:13.755065Z",
     "start_time": "2024-06-11T08:55:13.436220Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['sparse_categorical_accuracy'], label='accuracy')\n",
    "plt.plot(\n",
    "    history.history['val_sparse_categorical_accuracy'], label='val_accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.savefig(output_dir / \"normal_model_Accuracy.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aa5873a5a165aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T08:55:14.060342Z",
     "start_time": "2024-06-11T08:55:13.756535Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig(output_dir / \"normal_model_Loss.pdf\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
